{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, os, sys\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_cases(sentence):\n",
    "    sent_len = len(sentence)\n",
    "    num_changes = random.randint(int(sent_len/2), sent_len-2)\n",
    "    change_indices = []\n",
    "    for x in range(num_changes):\n",
    "        new_index = random.randint(0, sent_len-1)\n",
    "        if new_index not in change_indices:\n",
    "            change_indices.append(new_index)\n",
    "    for i in change_indices:\n",
    "        letter = sentence[i]\n",
    "        if letter.isupper() == True:\n",
    "            letter = letter.lower()\n",
    "        else:\n",
    "            letter = letter.upper()\n",
    "        sentence = ''.join([sentence[:i], letter, sentence[i+1:]])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batches(data, batch_len):\n",
    "    num_batches = int(len(data)/batch_len)\n",
    "    batches = [data[i:i+batch_len] for i in range(0, len(data), batch_len)]\n",
    "    ys = [data[i+batch_len+1] for i in range(0, len(data), batch_len)]\n",
    "    return batches, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_batch(data, batch_len):\n",
    "    start_index = random.randint(0, len(data)-batch_len-2)\n",
    "    batch = data[start_index:start_index+batch_len]\n",
    "    y = data[start_index+batch_len]\n",
    "    return batch, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_batch(data, vocab):\n",
    "    vec = np.zeros((len(data), len(vocab)))\n",
    "    for i, d in enumerate(data):\n",
    "        vec[i][vocab[d]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_item(item, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    vec[vocab[item]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unvectorize_item(one_hot, vocab_inv):\n",
    "    index = np.argmax(one_hot)\n",
    "    return vocab_inv[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unvectorize_batch(batch, vocab_inv):\n",
    "    ret = []\n",
    "    for b in batch:\n",
    "        index = np.argmax(b)\n",
    "        ret.append(vocab_inv[index])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class simple_rnn(nn.Module):\n",
    "    def __init__(self, input_len, rnn_size, num_layers, output_len):\n",
    "        super(simple_rnn, self).__init__()\n",
    "        self.gru = nn.GRU(input_len, rnn_size, num_layers)\n",
    "        self.out = nn.Linear(rnn_size, output_len)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x1, hidden  = self.gru(x)\n",
    "        x_flat = x1.reshape(x1.size()[1:])\n",
    "        x2 = self.out(x_flat)\n",
    "        x3 = F.softmax(x2, dim=1)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "sample_sentence = \"All work and no play makes Jack a dull boy. \"\n",
    "create_randomized_training_set = True\n",
    "train_file = \"train.txt\"\n",
    "num_sentences = 100\n",
    "train = sample_sentence\n",
    "if create_randomized_training_set == True:\n",
    "    for n in range(num_sentences):\n",
    "        randomized = randomize_cases(sample_sentence)\n",
    "        train += randomized\n",
    "with open(train_file, \"w\") as f:\n",
    "    f.write(train)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = []\n",
    "for n in range(len(train)-1):\n",
    "    letter = train[n]\n",
    "    if letter not in v:\n",
    "        v.append(letter)\n",
    "vocab = {}\n",
    "vocab_inv = {}\n",
    "for index, letter in enumerate(v):\n",
    "    vocab[letter] = index\n",
    "    vocab_inv[index] = letter\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, y = get_random_batch(train, 25)\n",
    "print(batch)\n",
    "print(\"y: \\\"\" + y + \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = vectorize_batch(batch, vocab)\n",
    "print(vecs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unvec = \"\"\n",
    "for v in vecs[0:10]:\n",
    "    test_unvec += unvectorize_item(v, vocab_inv)\n",
    "print(test_unvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(vecs[0])\n",
    "output_len = len(vecs[0])\n",
    "# Number of cells in each rnn layer\n",
    "rnn_size = 100\n",
    "# Number of layers\n",
    "num_layers = 3\n",
    "model = simple_rnn(input_len, rnn_size, num_layers, output_len)\n",
    "model = model.float()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvecs = torch.FloatTensor(vecs).unsqueeze(0)\n",
    "print(tvecs.shape)\n",
    "target = model(tvecs)[0]\n",
    "print(target)\n",
    "tnp = np.array(target.detach())\n",
    "print(unvectorize_item(tnp, vocab_inv))\n",
    "print(y)\n",
    "y_vec = torch.FloatTensor(vectorize_item(y, vocab))\n",
    "print(y_vec)\n",
    "loss = (y_vec - target).pow(2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# Maximum number of iterations to run\n",
    "num_iters = 100000\n",
    "# Number of characters to feed into rnn each iteration\n",
    "batch_len = 25\n",
    "# If we exceed this, we finish training\n",
    "target_accuracy = 0.95\n",
    "# A variable to record guesses, such that we can calculate accuracy as we train\n",
    "guesses = deque()\n",
    "losses = deque()\n",
    "acc = 0.0\n",
    "for iteration in range(num_iters):\n",
    "    batch, y = get_random_batch(train, batch_len)\n",
    "    vecs = vectorize_batch(batch, vocab)\n",
    "    tvecs = torch.FloatTensor(vecs).unsqueeze(0)\n",
    "    target = model(tvecs)\n",
    "    t_vec = target[-1].double()\n",
    "    y_vec = torch.tensor(vectorize_item(y, vocab), requires_grad=False, dtype=torch.float64)\n",
    "    tnp = target[-1].detach().numpy()\n",
    "    tch = unvectorize_item(tnp, vocab_inv)\n",
    "    loss = nn.MSELoss()(t_vec, y_vec)\n",
    "    corr = 0\n",
    "    if tch == y:\n",
    "        corr = 1\n",
    "    guesses.append(corr)\n",
    "    losses.append(float(loss))\n",
    "    if len(guesses) > 1000:\n",
    "        guesses.popleft()\n",
    "    if len(losses) > 1000:\n",
    "        losses.popleft()\n",
    "    if iteration % 1000 == 0:\n",
    "        if len(guesses) > 0:\n",
    "            correct = sum(guesses)\n",
    "        av_loss = np.mean(losses)\n",
    "        msg = \"Iter: \" + str(iteration) + \" Loss: \" + \"%.4f\"%float(av_loss) \n",
    "        msg += \" Correct: \" + str(correct) + \"/\" + str(len(guesses))\n",
    "        msg += \" Input: \\\"\" + batch + \"\\\" : \\\"\" \n",
    "        msg += \" Ouput: \\\"\" + tch + \"\\\" Expected: \\\"\" + y + \"\\\"\"\n",
    "        print(msg)\n",
    "        if acc > len(guesses)*0.95:\n",
    "            break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "output = \"\"\n",
    "start_char = random.sample(vocab.keys(), 1)[0]\n",
    "print(\"Starting character: \\\"\" + start_char + \"\\\"\")\n",
    "output += start_char\n",
    "start_vec = np.array(([vectorize_item(start_char, vocab)]))\n",
    "infer_len = 100\n",
    "t_sv = torch.FloatTensor(start_vec).unsqueeze(0)\n",
    "for n in range(infer_len):\n",
    "    next_vec = model(t_sv)\n",
    "    nv = np.array(next_vec[-1].detach())\n",
    "    tnv = unvectorize_item(nv, vocab_inv)\n",
    "    output += tnv\n",
    "    t_sv = next_vec.unsqueeze(0)\n",
    "print(\"\\\"\" + output + \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
